{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1R1gFFm4YSjRgDzR8fHlNF7mwEfZMJOMq",
      "authorship_tag": "ABX9TyOSIiDzTH/6sT8PJZSmJGhb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemasundar784/sms-spam-detection/blob/main/code_Sms_spam_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Reading data set\n",
        "sms_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/spam1.csv\", encoding=\"latin-1\")\n",
        "\n",
        "# Removing null columns\n",
        "sms_data.dropna(how=\"any\", inplace=True, axis=1)\n",
        "\n",
        "sms_data.columns = [\"label\", \"message\"]\n",
        "\n",
        "# Preprocessing the data\n",
        "def change(data):\n",
        "    extract_message = re.sub(\"[^a-zA-Z]\", \" \", data)\n",
        "    extract_message = extract_message.lower()\n",
        "    extract_message = extract_message.split()\n",
        "    extract_message = [lemmatizing.lemmatize(j) for j in extract_message if j not in stopwords.words('english')]\n",
        "    extract_message = \" \".join(extract_message)\n",
        "    return extract_message\n",
        "\n",
        "message_data = []\n",
        "lemmatizing = WordNetLemmatizer()\n",
        "\n",
        "for i in range(len(sms_data)):\n",
        "    message_data.append(change(sms_data[\"message\"][i]))\n",
        "\n",
        "# Change the data into numerical format\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform(message_data)\n",
        "\n",
        "# Dump the TfidfVectorizer\n",
        "pickle.dump(tfidf, open('transform.pkl', 'wb'))\n",
        "\n",
        "# Split the data for training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), sms_data['label'], test_size=0.3, random_state=0)\n",
        "print(len(X_train), len(X_test))\n",
        "\n",
        "# Multinomial Naive Bayes implementation\n",
        "class MultinomialNaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.class_prob = {}\n",
        "        self.word_prob = {}\n",
        "        self.classes = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        for label in self.classes:\n",
        "            self.class_prob[label] = np.sum(y == label) / len(y)\n",
        "            word_count = np.sum(X[y == label], axis=0)\n",
        "            self.word_prob[label] = (word_count + 1) / (np.sum(word_count) + len(word_count))\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for sample in X:\n",
        "            probs = []\n",
        "            for label in self.classes:\n",
        "                prob = np.log(self.class_prob[label]) + np.sum(np.log(self.word_prob[label][sample > 0]))\n",
        "                probs.append(prob)\n",
        "            predictions.append(self.classes[np.argmax(probs)])\n",
        "        return predictions\n",
        "\n",
        "# Create and fit the Multinomial Naive Bayes classifier\n",
        "mnb = MultinomialNaiveBayes()\n",
        "mnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the same classifier instance\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "# Dump the Multinomial Naive Bayes model\n",
        "pickle.dump(mnb, open('spam_model.pkl', 'wb'))\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report is:\")\n",
        "print(report)\n",
        "print(\"Accuracy is:\", accuracy * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlCzEYC8FRMC",
        "outputId": "cdd61cbb-6f78-4112-b72a-e343be238c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3990 1711\n",
            "Classification Report is:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      0.99      0.99      1454\n",
            "        spam       0.93      0.90      0.92       257\n",
            "\n",
            "    accuracy                           0.98      1711\n",
            "   macro avg       0.96      0.95      0.95      1711\n",
            "weighted avg       0.98      0.98      0.98      1711\n",
            "\n",
            "Accuracy is: 97.54529514903565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tem = \"your mobile number has own 1,000,000,00 pound from coca cola london to claim winning amount send your name/number/address/age to email@.com\"\n",
        "sample_message = change(tem)\n",
        "print(\"tem\", sample_message)\n",
        "l = tfidf.transform([sample_message]).toarray()\n",
        "\n",
        "# Calculate probabilities manually\n",
        "class_probs = []\n",
        "for label in mnb.classes:\n",
        "    class_prob = np.log(mnb.class_prob[label]) + np.sum(np.log(mnb.word_prob[label][l.flatten() > 0]))\n",
        "    class_probs.append(class_prob)\n",
        "\n",
        "class_probs = np.exp(class_probs - np.max(class_probs))  # Ensure numerical stability\n",
        "class_probs /= np.sum(class_probs)  # Normalize to get probabilities\n",
        "\n",
        "# Get the percentage of each class\n",
        "for i, label in enumerate(mnb.classes):\n",
        "    print(f\"Percentage of {label}: {class_probs[i] * 100:.2f}%\")\n",
        "\n",
        "print(\"Predicted Probabilities:\", class_probs)\n",
        "predicted_class = mnb.classes[np.argmax(class_probs)]\n",
        "print(\"Predicted Class:\", predicted_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M74Cp192GJa6",
        "outputId": "991e12b5-4731-4461-b53e-7bb6545ea043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tem mobile number pound coca cola london claim winning amount send name number address age email com\n",
            "Percentage of ham: 0.01%\n",
            "Percentage of spam: 99.99%\n",
            "Predicted Probabilities: [1.40008579e-04 9.99859991e-01]\n",
            "Predicted Class: spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vwlfroC_OX-",
        "outputId": "b2ed0f4e-7fe2-466e-9b06-58188ec23d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EKTh_54kCN4s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}