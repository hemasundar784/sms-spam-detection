{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemasundar784/sms-spam-detection/blob/main/code_of_MultinomialNB_algorithum\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4nQLivmhna7",
        "outputId": "7bf42e97-975f-4cb5-e6b2-c319ed4a527e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3990 1711\n",
            "Classification Report is:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.96      1.00      0.98      1454\n",
            "        spam       0.99      0.75      0.85       257\n",
            "\n",
            "    accuracy                           0.96      1711\n",
            "   macro avg       0.98      0.87      0.92      1711\n",
            "weighted avg       0.96      0.96      0.96      1711\n",
            "\n",
            "Accuracy is: 96.14260666277032\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import seaborn as sns\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "#reading data set\n",
        "sms_data=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/spam1.csv\",encoding=\"latin-1\")\n",
        "\n",
        "\n",
        "#removing null columns\n",
        "sms_data.dropna(how=\"any\",inplace=True,axis=1)\n",
        "\n",
        "sms_data.columns=[\"label\",\"message\"]\n",
        "\n",
        "#sns.countplot(x=\"label\",data=sms_data)\n",
        "#preprocessing the data\n",
        "def change(data):\n",
        "    extract_message=re.sub(\"[^a-zA-Z]\",\" \",data)\n",
        "    extract_message=extract_message.lower()\n",
        "    extract_message=extract_message.split()\n",
        "    extract_message=[lemmatizing.lemmatize(j) for j in extract_message if j not in stopwords.words('english')]\n",
        "    extract_message=\" \".join(extract_message)\n",
        "    return extract_message\n",
        "message_data=[]\n",
        "lemmatizing=WordNetLemmatizer()\n",
        "#stemming=PorterStemmer()\n",
        "for i in range(len(sms_data)):\n",
        "    message_data.append(change(sms_data[\"message\"][i]))\n",
        "\n",
        "\n",
        "#change the data into numerical format\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf=TfidfVectorizer()\n",
        "X=tfidf.fit_transform(message_data)\n",
        "#df=pd.DataFrame(X.toarray(),columns=tfidf.get_feature_names())\n",
        "#df.head(10)\n",
        "\n",
        "pickle.dump(tfidf,open('transform.pkl','wb'))\n",
        "\n",
        "# split the data for training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X.toarray(),sms_data['label'],test_size=0.3,random_state=0)\n",
        "print(len(X_train),len(X_test))\n",
        "\n",
        "#applying MultinomialNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mul=MultinomialNB().fit(X_train,y_train)\n",
        "y_pred=mul.predict(X_test)\n",
        "filename='spam_model.pkl'\n",
        "pickle.dump(mul,open(filename,'wb'))\n",
        "\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "accuracy=accuracy_score(y_test,y_pred)\n",
        "report=classification_report(y_test,y_pred)\n",
        "print(\"Classification Report is:\")\n",
        "print(report)\n",
        "print(\"Accuracy is:\",accuracy*100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "tem = \"your mobile number has own 1,000,000,00 pound from coca cola london to claim winning amount send your name/number/address/age to email@.com\"\n",
        "# Preprocess the sample message\n",
        "sample_message = change(tem)\n",
        "\n",
        "# Transform the sample message using the pre-trained tfidf transformer\n",
        "l = tfidf.transform([sample_message])\n",
        "\n",
        "# Predict the probability of being \"spam\" and \"ham\"\n",
        "probabilities = mul.predict_proba(l)\n",
        "\n",
        "# Calculate the percentage based on the probability of being \"spam\"\n",
        "percentage_spam = probabilities[0][1] * 100\n",
        "sample_pred=mul.predict(l)\n",
        "print(\"sample_pred\",sample_pred)\n",
        "\n",
        "print(f\"The percentage of the sample text that was classified as 'spam' is {percentage_spam:.2f}%.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTelEA8xH5Yp",
        "outputId": "ed38c80c-e1bd-4cfd-8ef4-d1ac65b13be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_pred ['spam']\n",
            "The percentage of the sample text that was classified as 'spam' is 61.64%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgCLZPYjv1yD",
        "outputId": "6fcc7d6f-f09f-4158-e70a-021f37754f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1XroZYxnn-w",
        "outputId": "57924ea6-8034-4a6f-a072-c7811944ffa9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1711"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aiPC-vlh2_U",
        "outputId": "f11aa063-e16e-457b-bee9-b7a359ac1374"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['spam'], dtype='<U4')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "sample_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCJYRVPDn3ze",
        "outputId": "6f441422-76d4-43c8-f076-14a3827ecd4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion_matrix is:\n",
            "[[1444   10]\n",
            " [  37  220]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "confusion=metrics.confusion_matrix(y_test,y_pred)\n",
        "print(\"confusion_matrix is:\")\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "r7Wy_mlZuOQM",
        "outputId": "06a5a907-c40a-455a-a428-3fe8d7f66d8f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# Sample text\\ntem = \"your mobile number has own 1,000,000,00 pound from coca cola london to claim winning amount send your name/number/address/age to email@.com \"\\n\\n# Preprocess the sample message\\nsample_message = change(tem)\\n\\n# Transform the sample message using the pre-trained tfidf transformer\\nl = tfidf.transform([sample_message])\\n\\n# Predict whether the sample message is \"ham\" or \"spam\"\\nsample_pred = MultinomialNB.predict(l)\\n\\n# Calculate the percentage based on the prediction\\npercentage_spam = 100 if sample_pred[0] == \\'spam\\' else 0\\n\\nprint(f\"The percentage of the sample text that was taken as \\'spam\\' is {percentage_spam}%.\")'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''# Sample text\n",
        "tem = \"your mobile number has own 1,000,000,00 pound from coca cola london to claim winning amount send your name/number/address/age to email@.com \"\n",
        "\n",
        "# Preprocess the sample message\n",
        "sample_message = change(tem)\n",
        "\n",
        "# Transform the sample message using the pre-trained tfidf transformer\n",
        "l = tfidf.transform([sample_message])\n",
        "\n",
        "# Predict whether the sample message is \"ham\" or \"spam\"\n",
        "sample_pred = MultinomialNB.predict(l)\n",
        "\n",
        "# Calculate the percentage based on the prediction\n",
        "percentage_spam = 100 if sample_pred[0] == 'spam' else 0\n",
        "\n",
        "print(f\"The percentage of the sample text that was taken as 'spam' is {percentage_spam}%.\")'''\n",
        "\n",
        "\n",
        "\n",
        "'''from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "# Create and fit the Bernoulli Naive Bayes classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the same classifier instance\n",
        "y_pred = bnb.predict(X_test)'''\n",
        "\n",
        "'''\n",
        "# own message prediction\n",
        "tem=\"your mobile number has own 1,000,000,00 pound from coca cola london to claim winning amount send your name/number/address/age to email@.com \"\n",
        "#tem1=\"Dear Customer, State Bank of India wishes you a Very Happy Birthday and a Great Year ahead!\"\n",
        "#tem=\"Dear 9398XXXX08, You have received Rs. 1 Paytm First Games Bonus. Use it to Play Games & win Paytm Cash. Install Now m.p-y.tm/GC_1 T&C\"\n",
        "sample_message=change(tem)\n",
        "print(\"tem\",sample_message)\n",
        "l=tfidf.transform([sample_message])\n",
        "sample_pred=mul.predict(l)\n",
        "print(\"sample_pred\",sample_pred)\n",
        "probabilities = mul.predict_proba(l)\n",
        "percentage_spam = probabilities[0][1] * 100 '''\n",
        "\n",
        "'''\n",
        "from sklearn import metrics\n",
        "confusion=metrics.confusion_matrix(y_test,y_pred)\n",
        "print(\"confusion_matrix is:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0ipz6H3oBRa",
        "outputId": "9413d51f-93a3-4343-b458-dfea9e167e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_pred ['spam']\n",
            "The percentage of the sample text that was classified as 'spam' is 100.00%.\n"
          ]
        }
      ],
      "source": [
        "# Sample text\n",
        "tem = \"Dear Reader,We have an exciting offer just for you! 🚀 Subscribe to Indian Express and stay informed with in-depth news, expert analysis, and exclusive content, all for less than Rs. 3 per day. 🗞️Heres what youll get:Daily insights on the stories that matter.Comprehensive analysis from our trusted experts.Exclusive access to premium content. Dont miss out on this opportunity to enhance your knowledge journey.Subscribe today and enjoy the best in news, insights, and expertise. 📚\"\n",
        "# Preprocess the sample message\n",
        "sample_message = change(tem)\n",
        "\n",
        "# Transform the sample message using the pre-trained tfidf transformer\n",
        "l = tfidf.transform([sample_message])\n",
        "\n",
        "# Predict the probability of being \"spam\" and \"ham\"\n",
        "probabilities = bnb.predict_proba(l)\n",
        "\n",
        "# Calculate the percentage based on the probability of being \"spam\"\n",
        "percentage_spam = probabilities[0][1] * 100\n",
        "sample_pred=bnb.predict(l)\n",
        "print(\"sample_pred\",sample_pred)\n",
        "\n",
        "print(f\"The percentage of the sample text that was classified as 'spam' is {percentage_spam:.2f}%.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmDIuGHHukQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed45d05e-b14c-4273-a8f0-c827449a59d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9684395090590298\n"
          ]
        }
      ],
      "source": [
        "#mul code\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.corpus import stopwords\n",
        "#from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "ps=WordNetLemmatizer()\n",
        "#ph=PorterStemmer()\n",
        "#print(stopwords.words('english'))\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/spam1.csv\",encoding=\"latin-1\")\n",
        "data.dropna(how=\"any\", inplace=True, axis=1)\n",
        "data.columns = ['label', 'message']\n",
        "#data.head(20)\n",
        "c=[]\n",
        "for i in range(len(data)):\n",
        "    review=re.sub(\"[^a-zA-Z]\",\" \",data['message'][i])\n",
        "    review=review.lower()\n",
        "    review=review.split()\n",
        "    review=[ps.lemmatize(j) for j in review if j not in stopwords.words('english')]\n",
        "    review=\" \".join(review)\n",
        "    c.append(review)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Create and fit the CountVectorizer\n",
        "tfidf = CountVectorizer()\n",
        "X = tfidf.fit_transform(c)\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "# Save the CountVectorizer\n",
        "pickle.dump(tfidf, open('transform.pkl', 'wb'))\n",
        "\n",
        "# Create DataFrame with feature names as columns\n",
        "df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X.toarray(),data['label'],test_size=0.3,random_state=0)\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mul=MultinomialNB().fit(X_train, y_train)\n",
        "y_pred=mul.predict(X_test)\n",
        "filename='spam_model.pkl'\n",
        "pickle.dump(mul,open(filename,'wb'))\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy=accuracy_score(y_test,y_pred)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import seaborn as sns\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "#reading data set\n",
        "sms_data=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/spam1.csv\",encoding=\"latin-1\")\n",
        "\n",
        "\n",
        "#removing null columns\n",
        "sms_data.dropna(how=\"any\",inplace=True,axis=1)\n",
        "\n",
        "sms_data.columns=[\"label\",\"message\"]\n",
        "\n",
        "\n",
        "#showing how many ham and spam messages in our data set\n",
        "['how','ham','spam']\n",
        "#sns.countplot(x=\"label\",data=sms_data)\n",
        "#preprocessing the data\n",
        "def change(data):\n",
        "    extract_message=re.sub(\"[^a-zA-Z]\",\" \",data)\n",
        "    extract_message=extract_message.lower()\n",
        "    extract_message=extract_message.split()\n",
        "    extract_message=[lemmatizing.lemmatize(j) for j in extract_message if j not in stopwords.words('english')]\n",
        "    extract_message=\" \".join(extract_message)\n",
        "    return extract_message\n",
        "message_data=[]\n",
        "lemmatizing=WordNetLemmatizer()\n",
        "#stemming=PorterStemmer()\n",
        "for i in range(len(sms_data)):\n",
        "    message_data.append(change(sms_data[\"message\"][i]))\n",
        "\n",
        "\n",
        "#change the data into numerical format\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf=TfidfVectorizer()\n",
        "X=tfidf.fit_transform(message_data)\n",
        "#df=pd.DataFrame(X.toarray(),columns=tfidf.get_feature_names())\n",
        "#df.head(10)\n",
        "\n",
        "pickle.dump(tfidf,open('transform.pkl','wb'))\n",
        "\n",
        "# split the data for training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X.toarray(),sms_data['label'],test_size=0.3,random_state=0)\n",
        "print(len(X_train),len(X_test))\n",
        "\n",
        "\n",
        "#from sklearn.naive_bayes import BernoulliNB\n",
        "'''\n",
        "# Create and fit the Bernoulli Naive Bayes classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the same classifier instance\n",
        "y_pred = bnb.predict(X_test)'''\n",
        "\n",
        "\n",
        "#applying MultinomialNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "MultinomialNB=MultinomialNB().fit(X_train,y_train)\n",
        "y_pred=MultinomialNB.predict(X_test)\n",
        "\n",
        "pickle.dump(MultinomialNB,open('model.pkl','wb'))\n",
        "\n",
        "# own message prediction\n",
        "tem=\"your mobile number has own 1,000,000,00 pound from coca cola london to claim winning amount send your name/number/address/age to email@.com \"\n",
        "#tem1=\"Dear Customer, State Bank of India wishes you a Very Happy Birthday and a Great Year ahead!\"\n",
        "#tem=\"Dear 9398XXXX08, You have received Rs. 1 Paytm First Games Bonus. Use it to Play Games & win Paytm Cash. Install Now m.p-y.tm/GC_1 T&C\"\n",
        "sample_message=change(tem)\n",
        "print(\"tem\",sample_message)\n",
        "l=tfidf.transform([sample_message])\n",
        "sample_pred=MultinomialNB.predict(l)\n",
        "print(\"sample_pred\",sample_pred)\n",
        "probabilities = MultinomialNB.predict_proba(l)\n",
        "percentage_spam = probabilities[0][1] * 100\n",
        "\n",
        "'''\n",
        "from sklearn import metrics\n",
        "confusion=metrics.confusion_matrix(y_test,y_pred)\n",
        "print(\"confusion_matrix is:\")\n",
        "print(confusion)\n",
        "'''\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "accuracy=accuracy_score(y_test,y_pred)\n",
        "report=classification_report(y_test,y_pred)\n",
        "print(\"Classification Report is:\")\n",
        "print(report)\n",
        "print(\"Accuracy is:\",accuracy*100)\n",
        "print(f\"The percentage of the sample text that was classified as 'spam' is {percentage_spam:.2f}%.\")\n",
        "\n",
        "\n",
        "sample_pred\n",
        "len(y_pred)\n",
        "# Sample text\n",
        "tem = 'hello this is hemasundadr'\n",
        "# Preprocess the sample message\n",
        "sample_message = change(tem)\n",
        "\n",
        "# Transform the sample message using the pre-trained tfidf transformer\n",
        "l = tfidf.transform([sample_message])\n",
        "\n",
        "# Predict the probability of being \"spam\" and \"ham\"\n",
        "probabilities = MultinomialNB.predict_proba(l)\n",
        "\n",
        "# Calculate the percentage based on the probability of being \"spam\"\n",
        "percentage_spam = probabilities[0][1] * 100\n",
        "sample_pred=MultinomialNB.predict(l)\n",
        "print(\"sample_pred\",sample_pred)\n",
        "\n",
        "print(f\"The percentage of the sample text that was classified as 'spam' is {percentage_spam:.2f}%.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH20d8VbNih8",
        "outputId": "abc5a2e3-8950-4966-985f-066fc0af6490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3990 1711\n",
            "tem mobile number pound coca cola london claim winning amount send name number address age email com\n",
            "sample_pred ['spam']\n",
            "Classification Report is:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.96      1.00      0.98      1454\n",
            "        spam       0.99      0.75      0.85       257\n",
            "\n",
            "    accuracy                           0.96      1711\n",
            "   macro avg       0.98      0.87      0.92      1711\n",
            "weighted avg       0.96      0.96      0.96      1711\n",
            "\n",
            "Accuracy is: 96.14260666277032\n",
            "The percentage of the sample text that was classified as 'spam' is 61.64%.\n",
            "sample_pred ['ham']\n",
            "The percentage of the sample text that was classified as 'spam' is 7.19%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "tem = 'hello this is hemasundadr'\n",
        "# Preprocess the sample message\n",
        "sample_message = change(tem)\n",
        "\n",
        "# Transform the sample message using the pre-trained tfidf transformer\n",
        "l = tfidf.transform([sample_message])\n",
        "\n",
        "# Predict the probability of being \"spam\" and \"ham\"\n",
        "probabilities = MultinomialNB.predict_proba(l)\n",
        "\n",
        "# Calculate the percentage based on the probability of being \"spam\"\n",
        "percentage_spam = probabilities[0][1] * 100\n",
        "sample_pred=MultinomialNB.predict(l)\n",
        "print(\"sample_pred\",sample_pred)\n",
        "\n",
        "print(f\"The percentage of the sample text that was classified as 'spam' is {percentage_spam:.2f}%.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsgI7eOKQ2U7",
        "outputId": "fc0a8ae0-86d0-4785-d66b-c08472920b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_pred ['ham']\n",
            "The percentage of the sample text that was classified as 'spam' is 7.19%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8dWvx9NZRyv7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1gErsE0yiQgljMzV-HrPaaMbb2v3FxeW3",
      "authorship_tag": "ABX9TyNjNuzJlS2HgO6/JceLcyfD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}